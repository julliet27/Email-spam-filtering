# -*- coding: utf-8 -*-
"""Email Spam Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11a8K9HNnjGaZ14-uhfMBPW6K_bogIJvG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import feature_extraction, model_selection, naive_bayes, metrics
from collections import Counter
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,fbeta_score
from sklearn.datasets import load_iris
from scipy import stats
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

feature = pd.read_csv('test_features.csv')
train=pd.read_csv('train_data.csv')

train.isnull().sum()  #NaN

count_Class=pd.value_counts(train["ham"], sort= True)
count_Class.plot(kind= 'bar', color= ["green", "red"])
plt.title('HAM')
plt.show()

train = train.drop(columns = ['capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total', 'Id'])

HAMdata = train[train['ham']==True]
SPAMdata = train[train['ham']==False]

HAMdata.drop(columns = ['ham'])
SPAMdata.drop(columns = ['ham'])

HAMrow = HAMdata.shape[0]
SPAMrow= SPAMdata.shape[0]
ALLrow = SPAMrow + HAMrow
SPAMperc = SPAMrow * 100 / ALLrow
HAMperc = HAMrow* 100 / ALLrow
print(HAMperc," ",SPAMperc)

sums1 = HAMdata.select_dtypes(pd.np.number).sum().rename('total')

sums2 = SPAMdata.select_dtypes(pd.np.number).sum().rename('total')

sums1.plot.bar(legend = False)
plt.title('Ham messages')
plt.xlabel('Words or Chars')
plt.ylabel('Numbers')
plt.show()

sums2.plot.bar(legend = False)
plt.title('spam messages')
plt.xlabel('Words or Chars')
plt.ylabel('Numbers')
plt.show()

train.drop(columns = ['word_freq_you'])

sums1FRACTION = sums1 / HAMrow
sums2FRACTION = sums2 / SPAMrow
difSums = sums2FRACTION - sums1FRACTION
#print(difSums)

difSums.plot.bar(legend = False)
plt.title('Difference between frequencies of words and chars')
plt.xlabel('Words or Chars')
plt.ylabel('Numbers')
plt.show()

train_corr = train.corr()
train_corr

import seaborn as sns
sns.heatmap(train_corr, cmap = 'YlGnBu')

outputDATA = train['ham']
inputDATA = train.drop(columns=['ham'])
X_train, X_test, y_train, y_test = train_test_split(inputDATA,outputDATA,random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

gnb = GaussianNB()
gnb.fit(X_train, y_train)
predictions = gnb.predict(X_train)
print ('Naive Bayes: ' ,accuracy_score(y_train, predictions))

cm = confusion_matrix(y_test, gnb.predict(X_test))
TN = cm[0][0]
TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]
value=(TP + TN) / (TP + TN + FN + FP)
print(cm)
print('Model Testing Accuracy: ',value)

irisData = load_iris()
X = irisData.data
y = irisData.target
  
neighbors = np.arange(1, 9)
lst=[]

for i,k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    lst.append(knn.score(X_test, y_test))
print("K-nearest neighbors: ",min(lst))
cm = confusion_matrix(y_test, knn.predict(X_test))
TN = cm[0][0]
TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]
value=(TP + TN) / (TP + TN + FN + FP)
print(cm)
print('Model Testing Accuracy: ',value)

clf = DecisionTreeClassifier(criterion='entropy',random_state=1)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
score=accuracy_score(y_pred,y_test)
print("Decision Tree: ",score)
cm = confusion_matrix(y_test, clf.predict(X_test))
TN = cm[0][0]
TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]
value=(TP + TN) / (TP + TN + FN + FP)
print(cm)
print('Model Testing Accuracy: ',value)

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state = 0)
log.fit(X_train, y_train)
print('Logistic Regression: ', log.score(X_train, y_train))
cm = confusion_matrix(y_test, log.predict(X_test))
TN = cm[0][0]
TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]
value=(TP + TN) / (TP + TN + FN + FP)
print(cm)
print('Model Testing Accuracy: ',value)